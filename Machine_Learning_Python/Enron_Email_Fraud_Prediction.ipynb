{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Enron Email Fraud Detection Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron was an American energy, commodities, and services company that went bankrupt in 2001 due to widespread financial fraud and corruption. As a result of Federal investigations, tens of thousands of emails and financial data were released to the public. Hence, this project investigated the Enron Email dataset and developed a 'person of interest' (POI) prediction model. Financial and email features were selected via kbest, decision tree, and lasso regularization. Nine machine learning classification prediction algorithms were investigated: Logistic Regression, Naive-Bayes, K-means Clustering, K-nearest Neighbors, Random Forest, Extra Tree, Gradient Boosting, Decision Tree, and Adaboost. Adaboost yielded the best results. The final tuned Adaboost model exhibited 85% accuracy, 43% precision, and 34% recall. Validation was preformed by 1000-fold stratified shuffle-split cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Source: https://github.com/udacity/ud120-projects.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was initially stored in a dictionary, where each key-value pair corresponded to each observation or person name. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contained 146 observations (people) consisting of 14 financial features ('salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'), 6 email features (to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'), and 1 label/outcome ('poi'). Of the 146 observations, 18 were labeled as a 'person of interest' (12%) and 128 were labeled as 'not a person of interest' (88%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> data_dict['LAY KENNETH L']\n",
    "{'bonus': 7000000,\n",
    " 'deferral_payments': 202911,\n",
    " 'deferred_income': -300000,\n",
    " 'director_fees': 'NaN',\n",
    " 'email_address': 'kenneth.lay@enron.com',\n",
    " 'exercised_stock_options': 34348384,\n",
    " 'expenses': 99832,\n",
    " 'from_messages': 36,\n",
    " 'from_poi_to_this_person': 123,\n",
    " 'from_this_person_to_poi': 16,\n",
    " 'loan_advances': 81525000,\n",
    " 'long_term_incentive': 3600000,\n",
    " 'other': 10359729,\n",
    " 'poi': True,\n",
    " 'restricted_stock': 14761694,\n",
    " 'restricted_stock_deferred': 'NaN',\n",
    " 'salary': 1072321,\n",
    " 'shared_receipt_with_poi': 2411,\n",
    " 'to_messages': 4273,\n",
    " 'total_payments': 103559793,\n",
    " 'total_stock_value': 49110078}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number and fraction of missing values for each feature was investigated. The features with more than 50% missing values were 'long_term_incentive', 'deferred_income', 'deferral_payments', 'restricted_stock_deferred', 'director_fees', and 'loan_advances'. All missing values were converted to 0 in order to perform machine learning operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'poi' 0, 0.0\n",
    "'total_stock_value' 20, 0.14\n",
    "'total_payments' 21, 0.14\n",
    "'email_address' 35, 0.24\n",
    "'restricted_stock' 36, 0.25\n",
    "'exercised_stock_options' 44, 0.3\n",
    "'expenses' 51, 0.35\n",
    "'salary' 51, 0.35\n",
    "'other' 53, 0.36\n",
    "'from_messages' 60, 0.41\n",
    "'from_poi_to_this_person' 60, 0.41\n",
    "'from_this_person_to_poi' 60, 0.41\n",
    "'shared_receipt_with_poi' 60, 0.41\n",
    "'to_messages' 60, 0.41\n",
    "'bonus' 64, 0.44\n",
    "'long_term_incentive' 80, 0.55\n",
    "'deferred_income' 97, 0.66\n",
    "'deferral_payments' 107, 0.73\n",
    "'restricted_stock_deferred' 128, 0.88\n",
    "'director_fees' 129, 0.88\n",
    "'loan_advances' 142, 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three observations were removed from the dataset:'THE TRAVEL AGENCY IN THE PARK', 'TOTAL', and 'LOCKHART EUGENE E'. The 'THE TRAVEL AGENCY IN THE PARK' and 'TOTAL' observations are not individual person observations. Moreover, the 'LOCKHART EUGENE E' observation was removed because it only contained missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature Engineering  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two new features were created: 'from_poi_percent' and 'to_poi_percent'. The feature 'from_poi_percent' was calculated by dividing 'from_poi' by 'to_messages'. Similarly, 'to_poi_percent' was calculated by dividing 'to_poi' by 'from_messages'. These features were created to obtain a ratio of the number of messages from and to persons of interest. These ratios go beyond the raw message numbers and give us more insight into the fraction/ratio of messages to and from persons of interest. I hypothesized that high/low ratios signify high/low contact with persons of interest better than raw message numbers that can vary significantly between people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features were selected by 3 processes: kbest, decision tree, and lasso regularization algorithms. The kbest features were selected by p-values less than 0.15. The decision tree best features were selected by feature importances scores greater than 0. The lasso regularization best features were selected by coefficients greater than 0. The best features (total of 13 unique features) across all of these processes were collected and used for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, all features were scaled prior to feature selection and machine learning. Scaling was conducted in order to equally weight all features in some machine learning algorithms, such as K-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> kbest_features:\n",
    "'bonus', 0.02\n",
    "'exercised_stock_options', 0.01\n",
    "'loan_advances', 0.01\n",
    "'long_term_incentive', 0.11\n",
    "'salary', 0.08\n",
    "'shared_receipt_with_poi', 0.12\n",
    "'total_payments', 0.1\n",
    "'total_stock_value', 0.02\n",
    "'to_poi_percent', 0.03\n",
    "'to_poi_percent', 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> decision_best: \n",
    "'exercised_stock_options', 0.2\n",
    "'expenses', 0.12\n",
    "'long_term_incentive', 0.04\n",
    "'other', 0.13\n",
    "'restricted_stock', 0.03\n",
    "'shared_receipt_with_poi', 0.18\n",
    "'total_payments', 0.07\n",
    "'total_stock_value', 0.1\n",
    "'to_poi_percent', 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> lasso_best:\n",
    "'bonus', 0.28\n",
    "'deferred_income', 0.14\n",
    "'exercised_stock_options', 0.26\n",
    "'expenses', 0.01\n",
    "'loan_advances', 0.01\n",
    "'long_term_incentive', 0.01\n",
    "'restricted_stock', 0.01\n",
    "'salary', 0.16\n",
    "'total_payments', 0.01\n",
    "'total_stock_value', 0.2\n",
    "'to_poi_percent', 0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithms were tested: Logistic Regression, Naive-Bayes, K-means Clustering, K-nearest Neighbors, Random Forest, Extra Tree, Gradient Boosting, Decision Tree, and Adaboost. These algorithms were compared via accuracy, precision, recall, and F1 score. The algorithm that was selected was Adaboost, which had the best combined precision, recall, and FI score results (all greater than 0.3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logistic Regression:  Accuracy: 0.80073,  Precision: 0.20791,  Recall: 0.17600,  F1: 0.19063\n",
    "Naive-Bayes:          Accuracy: 0.81293,  Precision: 0.30303,  Recall: 0.31000,  F1: 0.30648\n",
    "K-means Clustering:   Accuracy: 0.83433,  Precision: 0.22783,  Recall: 0.10150,  F1: 0.14044\n",
    "K-nearest Neighbors:  Accuracy: 0.86147,  Precision: 0.38596,  Recall: 0.06600,  F1: 0.11272\n",
    "Random Forest:        Accuracy: 0.86140,  Precision: 0.43577,  Recall: 0.13400,  F1: 0.20497\n",
    "Extra Tree:           Accuracy: 0.85867,  Precision: 0.41254,  Recall: 0.14150,  F1: 0.21072\n",
    "Gradient Boosting:    Accuracy: 0.85947,  Precision: 0.45408,  Recall: 0.26700,  F1: 0.33627\n",
    "Decision Tree:        Accuracy: 0.82407,  Precision: 0.33823,  Recall: 0.33400,  F1: 0.33610\n",
    "Adaboost:             Accuracy: 0.84687,  Precision: 0.40782,  Recall: 0.32850,  F1: 0.36389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm tuning is the process in which model hyper-parameter are selected for a specific problem/dataset. Hyper-parameters are algorithm-specific variables such as alphas, gammas, number of estimators, learning rate, max_depth, and kernals. Algorithms hyper-parameters are initially set to default values, which may or may not be appropriate for the current problem.  Algorithm tuning is important in order to improve/optimize/maximize performance (i.e. accuracy, precision, and recall) and utilize hyper-parameters that are appropriate for the underlying problem/dataset. Tuning is a search process by using either grid search (trial and error) or random search. If a model is not tuned well, the accuracy, precision, and/or recall will not be optimal. Moreover, there is a trade-off between recall and precision (inverse relationship) that is strongly dependent on the tuning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm was tuned to improve precision and recall. It was tuned by conducting a cross-validated grid search of the model hyper-parameters (n_estimators, learning_rate, and algorithm). In simple terms, an exhaustive trial-and-error search was conducted to find the hyper-parameters that yield the best results. In this case, the F1 score was maximized in order to stress both precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best AdaBoost parameters were found to be n_estimators = 100, learning_rate = 0.9, and algorithm = SAMME.R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Adaboost (Tuned):    Accuracy: 0.85167,  Precision: 0.42866,  Recall: 0.33800,  F1: 0.37797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously engineered feature 'to_poi_percent' was found to be the second/third most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> ada_best      \n",
    "'bonus', 0.09\n",
    "'deferred_income', 0.05\n",
    "'exercised_stock_options', 0.13\n",
    "'expenses', 0.22\n",
    "'long_term_incentive', 0.06\n",
    "'other', 0.07\n",
    "'restricted_stock', 0.03\n",
    "'salary', 0.1\n",
    "'shared_receipt_with_poi', 0.05\n",
    "'total_payments', 0.04\n",
    "'total_stock_value', 0.03\n",
    "'to_poi_percent', 0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main evaluation metrics for the final tuned Adaboost model were precision (43%) and recall (34%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the ratio of true positives over the sum of true positives and false positives (positive predictive value). In simple terms, this means that 43% of the predicted persons of interest were actually persons of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is the ratio of true positives over the sum of true positives and false negatives (true positive rate). In simple terms, this means that 34% of persons of interest were predicted to be persons of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Accuracy was not a major consideration because of the small and unbalanced dataset (146 observations) with only 18 persons of interest. The accuracy of the baseline model (all considered not a person of interest) was 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is a testing process used to asses model performance. Various validation methods exist, such as hold-out data, cross-validation, and bootstrapping. Validation is typically conducted by separating data into training and testing datasets (i.e. 80% training and 20% testing). In order to accurately determine model performance, validation tests are carried out on data that is independent from data that used during training. The classic mistake is over-fitting to the training dataset. It is important to use validation in order to avoid over-fitting and improve model performance on non-training data. If validation is not implemented, over-fitting results in maximal results on the training dataset, but poor results (i.e. accuracy) on the testing dataset. Moreover, the testing dataset should not be utilized until the final model has been selected/tuned. The testing dataset should not be manipulated or run multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis was validated by the following: 1.) simple data splitting into training and test datasets (1:1 ratio due to small and unbalanced dataset) via sklearn.cross_validation.train_test_split, and 2.) stratified shuffle-split cross-validation (1000 folds) via sklearn.cross_validation.StratifiedShuffleSplit (as developed by Udacity in the tester.py file). The stratified shuffle-split cross-validation is the best solution for this project, due to the small and unbalanced dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"./tools/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Inport Data -------------------------------------------------------\n",
    "\n",
    "# features_list is a list of strings, each of which is a feature name\n",
    "features_list = ['poi', \n",
    "    'bonus', 'deferral_payments', 'deferred_income', 'director_fees',\n",
    "    'exercised_stock_options', 'expenses', 'from_messages',\n",
    "    'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances',\n",
    "    'long_term_incentive', 'other', 'restricted_stock',\n",
    "    'restricted_stock_deferred', 'salary', 'shared_receipt_with_poi',\n",
    "    'to_messages', 'total_payments', 'total_stock_value' ]\n",
    "feature_names = features_list[1:]\n",
    "\n",
    "# load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# allocation of labels     \n",
    "labels_poi = []\n",
    "for x in data_dict:\n",
    "    labels_poi.append( data_dict[x]['poi'] )\n",
    "print labels_poi.count(0)\n",
    "print labels_poi.count(1)\n",
    "\n",
    "# missing values counts\n",
    "values = data_dict.values()[0].keys()\n",
    "values_dict = {}\n",
    "for x in values:\n",
    "    values_dict[x] = 0\n",
    "for x in range( 0,len( data_dict.values() ) ):\n",
    "    for y in values:\n",
    "        if data_dict.values()[x][y] == 'NaN':\n",
    "            values_dict[y] = values_dict[y] + 1\n",
    "print values_dict\n",
    "for key, value in sorted(values_dict.iteritems(), key=lambda (k,v): (v,k)):\n",
    "    print key, value, round( float(value)/len( data_dict.values() ), 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 2: Remove Outliers ---------------------------------------------------\n",
    "\n",
    "# remove outlier entries \n",
    "data_dict_out = data_dict.copy()\n",
    "del data_dict_out['THE TRAVEL AGENCY IN THE PARK']\n",
    "del data_dict_out['TOTAL']\n",
    "del data_dict_out['LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 3-1: Create New Features ---------------------------------------------\n",
    "\n",
    "# create features for percent of to/from messages \n",
    "data_dict_new = data_dict_out.copy()\n",
    "for each in data_dict_new:\n",
    "    from_poi = float( data_dict_new[each]['from_poi_to_this_person'] )\n",
    "    to_poi = float( data_dict_new[each]['from_this_person_to_poi'] ) \n",
    "    from_messages = float( data_dict_new[each]['from_messages'] )\n",
    "    to_messages = float( data_dict_new[each]['to_messages'] )\n",
    "    # add new features \n",
    "    data_dict_new[each]['from_poi_percent'] = from_poi / to_messages\n",
    "    data_dict_new[each]['to_poi_percent'] = to_poi / from_messages\n",
    "    # fix nan values that will cause errors, convert to string 'NaN'\n",
    "    if math.isnan( data_dict_new[each]['from_poi_percent'] ):\n",
    "        data_dict_new[each]['from_poi_percent'] = 'NaN'\n",
    "    if math.isnan( data_dict_new[each]['to_poi_percent'] ):\n",
    "        data_dict_new[each]['to_poi_percent'] = 'NaN'\n",
    "        \n",
    "# add new features to features_list\n",
    "features_list.append('from_poi_percent')\n",
    "features_list.append('to_poi_percent')\n",
    "feature_names = features_list[1:]\n",
    "\n",
    "# store to my_dataset for export \n",
    "my_dataset = data_dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 3-2: Feature Selection -----------------------------------------------\n",
    "\n",
    "# extract all features and labels from dataset\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# preprocess all features \n",
    "from sklearn import preprocessing\n",
    "scale = preprocessing.MinMaxScaler()\n",
    "features = scale.fit_transform(features)\n",
    "\n",
    "# kbest feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "featureSelector = SelectKBest(chi2, k='all')\n",
    "featureSelector.fit(features, labels)\n",
    "featureSelector.pvalues_\n",
    "kbest_features = []\n",
    "for x in range(0,len(feature_names)):\n",
    "    if featureSelector.pvalues_[x] < 0.15:\n",
    "        kbest_features.append([feature_names[x],\n",
    "                    round(featureSelector.pvalues_[x],2)])\n",
    "print kbest_features        \n",
    "        \n",
    "# decision tree variable importance \n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features, labels)\n",
    "clf.feature_importances_\n",
    "decision_best = []\n",
    "for x in range(0,len(feature_names)):\n",
    "    if clf.feature_importances_[x] > 0:\n",
    "        decision_best.append([feature_names[x],\n",
    "                    round(clf.feature_importances_[x],2)])\n",
    "print decision_best       \n",
    "\n",
    "# Lasso regularization feature selection \n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "rlasso = RandomizedLasso(alpha=0.0085)\n",
    "rlasso.fit(features, labels)\n",
    "lasso_best = []\n",
    "for x in range(0,len(feature_names)):\n",
    "    if rlasso.scores_[x] > 0:\n",
    "        lasso_best.append([feature_names[x],\n",
    "                            round(rlasso.scores_[x],2)])\n",
    "print lasso_best\n",
    "\n",
    "# top features selected\n",
    "features_list = ['poi', \n",
    "        'bonus', 'deferred_income', 'exercised_stock_options',\n",
    "        'expenses', 'loan_advances', 'long_term_incentive',\n",
    "        'other', 'restricted_stock', 'salary',\n",
    "        'shared_receipt_with_poi', 'total_payments', \n",
    "        'total_stock_value', 'to_poi_percent'] \n",
    "feature_names = features_list[1:]\n",
    "\n",
    "# extract only selected features and labels from dataset\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# preprocess selected features \n",
    "from sklearn import preprocessing\n",
    "scale = preprocessing.MinMaxScaler()\n",
    "features = scale.fit_transform(features)\n",
    "\n",
    "# split data into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 4: Test Classifiers --------------------------------------------------\n",
    "\n",
    "# logistic regression\n",
    "# Accuracy: 0.80073\tPrecision: 0.20791\tRecall: 0.17600\t\n",
    "# F1: 0.19063\tF2: 0.18157\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# naive-bayes \n",
    "# Accuracy: 0.81293\tPrecision: 0.30303\tRecall: 0.31000\t\n",
    "# F1: 0.30648\tF2: 0.30858\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "# k-means clustering \n",
    "# Accuracy: 0.83433\tPrecision: 0.22783\tRecall: 0.10150\t\n",
    "# F1: 0.14044\tF2: 0.11416\n",
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=2)\n",
    "\n",
    "# k-nearest neighbors \n",
    "# Accuracy: 0.86147\tPrecision: 0.38596\tRecall: 0.06600\t\n",
    "# F1: 0.11272\tF2: 0.07912\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# random forest\n",
    "# Accuracy: 0.86140\tPrecision: 0.43577\tRecall: 0.13400\t\n",
    "# F1: 0.20497\tF2: 0.15554\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# extra tree\n",
    "# Accuracy: 0.85867\tPrecision: 0.41254\tRecall: 0.14150\t\n",
    "# F1: 0.21072\tF2: 0.16291\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier()\n",
    "\n",
    "# gradient-boosting \n",
    "# Accuracy: 0.85947\tPrecision: 0.45408\tRecall: 0.26700\t\n",
    "# F1: 0.33627\tF2: 0.29098\n",
    "from sklearn import ensemble\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "# decision tree\n",
    "# Accuracy: 0.82407\tPrecision: 0.33823\tRecall: 0.33400\t\n",
    "# F1: 0.33610\tF2: 0.33484\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# ada-boosting \n",
    "# Accuracy: 0.84687\tPrecision: 0.40782\tRecall: 0.32850\t\n",
    "# F1: 0.36389\tF2: 0.34180\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 5-1: Tune Classifier -------------------------------------------------\n",
    "\n",
    "# cross-validation of training data\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "cv = ShuffleSplit(features_train.shape[0], \n",
    "                    n_iter=5, test_size=0.2, random_state=42)\n",
    "\n",
    "# tune decision tree classifier via grid search cv\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "param_grid = { 'criterion': ['gini','entropy'],\n",
    "               'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "               'splitter' : ['best','random'],\n",
    "               'max_depth' : [None,1,2,4,8,10,15],\n",
    "               'min_samples_split' : [1,2,3,5,10],\n",
    "               'min_samples_leaf' : [1,2,3],\n",
    "               'class_weight' : [None, 'balanced']}\n",
    "cv_clf = GridSearchCV(estimator=clf, \n",
    "                      cv=cv, \n",
    "                      param_grid=param_grid,\n",
    "                      scoring='f1')\n",
    "cv_clf.fit(features_train, labels_train)\n",
    "print cv_clf.best_params_\n",
    "\n",
    "# tune decision tree classifier via grid search cv\n",
    "# Accuracy: 0.81427\tPrecision: 0.33827\tRecall: 0.41100\t\n",
    "# F1: 0.37111\tF2: 0.39406\n",
    "clf = tree.DecisionTreeClassifier(splitter = 'best', \n",
    "            min_samples_leaf = 2,\n",
    "            min_samples_split = 2, \n",
    "            criterion = 'entropy', \n",
    "            max_features = 'log2', \n",
    "            max_depth = 8, \n",
    "            class_weight = 'balanced')\n",
    "\n",
    "# tune ada-boosting classifier via grid search cv\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "param_grid = { 'n_estimators': [50,100,200],\n",
    "               'learning_rate': [0.7,0.9,1,1.1],\n",
    "               'algorithm' : ['SAMME', 'SAMME.R']}\n",
    "cv_clf = GridSearchCV(estimator=clf, \n",
    "                      cv=cv, \n",
    "                      param_grid=param_grid,\n",
    "                      scoring = 'f1')\n",
    "cv_clf.fit(features_train, labels_train)\n",
    "print cv_clf.best_params_\n",
    "\n",
    "# tuned ada-boosting model via grid search cv\n",
    "# 1st run: Accuracy: 0.85167  Precision: 0.42866  Recall: 0.33800\t\n",
    "# F1: 0.37797\tF2: 0.35293\n",
    "# 2nd run: Accuracy: 0.85153  Precision: 0.42803  Recall: 0.33750\t\n",
    "# F1: 0.37741\tF2: 0.35241\n",
    "clf = AdaBoostClassifier(n_estimators = 100, \n",
    "                         learning_rate = 0.9,\n",
    "                         algorithm = 'SAMME.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 5-2: Evaluation Metrics ----------------------------------------------\n",
    "\n",
    "# predictions\n",
    "clf.fit(features_train, labels_train)\n",
    "test_predictions = clf.predict(features_test)\n",
    "\n",
    "# feature importance \n",
    "clf.feature_importances_\n",
    "ada_best = []\n",
    "for x in range(0,len(feature_names)):\n",
    "    if clf.feature_importances_[x] > 0:\n",
    "        ada_best.append([feature_names[x],\n",
    "                    round(clf.feature_importances_[x],2)])\n",
    "print ada_best      \n",
    "\n",
    "# confusion matrix and errors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "print confusion_matrix(labels_test, test_predictions) \n",
    "print classification_report(labels_test, test_predictions)\n",
    "print roc_auc_score(labels_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 6: Dump Classifier, Dataset, and Features --------------------------\n",
    "\n",
    "from tester import dump_classifier_and_data\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
